<h1>Cache Performance Metrics and Multilevel Cache Hierarchies</h1>
<p style="color:whitesmoke;">
<strong>Performance Considerations</strong> continue to play a central role in memory hierarchy design. As <strong>CPU clock rates</strong> rise without proportional improvement in memory systems, the fraction of time spent on <strong>memory stalls</strong>—cycles where the processor waits for data—becomes a larger component of total execution time. This is quantified by <strong>Amdahl's Law</strong>, which describes how the portion of a system that cannot be improved (in this case, memory stalls) limits the system's speedup.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
In the <strong>Cache Performance Example Revisit</strong>, if the system's <strong>cycles per instruction (CPI)</strong> is reduced from 2 to 1 without altering the <strong>clock rate</strong>, the <strong>actual CPI</strong> becomes <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1 + 3.44 = 4.44</code> due to persistent memory stalls. The ideal CPU, in this case, would be <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">4.44/1 = 4.44</code> times faster, but the portion of time spent stalled on memory accesses increases from <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">3.44/5.44 = 63%</code> to <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">3.44/4.44 = 77%</code>. This demonstrates how improvements in CPU core performance amplify the impact of slow memory. 
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Performance Considerations</strong> also extend to <strong>clock rate scaling</strong>. As <strong>clock rates</strong> rise and the <strong>memory system</strong> does not correspondingly speed up, the fraction of cycles lost to <strong>cache misses</strong> increases, leading to more pronounced <strong>memory stalls</strong>.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
The <strong>Hit Access Time</strong> is the time to access a word from the memory system when it is present in the cache (<strong>cache hit</strong>). If this <strong>hit time</strong> increases, overall system performance suffers, even if the miss rate is low. Several factors influence <strong>hit access time</strong>:
<ul>
  <li><strong>Cache size</strong>: Larger caches generally have longer hit times due to increased internal communication paths.</li>
  <li><strong>Number of pipeline stages</strong>: Increasing stages can mitigate longer hit times, but introduces pipeline management complexity.</li>
  <li><strong>Cache organization</strong>: Direct-mapped, set associative, and fully associative caches offer different tradeoffs in terms of parallelism and search complexity, impacting hit time.</li>
</ul>
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Multilevel Caches</strong> are implemented to mitigate access time and miss penalty tradeoffs. The <strong>primary cache (L1)</strong> is directly attached to the CPU and designed for minimal hit time, but its small size means some data will miss and must be retrieved from lower levels. The <strong>secondary (L2) cache</strong> is larger but slower and services L1 misses while striving for a low miss rate. Main memory then services the L2 misses—much slower than cache accesses. In high-end systems, a <strong>Level-3 (L3) cache</strong> may also be present, extending the hierarchy. Each level presents a <strong>tradeoff between hit time, miss rate, and block size</strong>.
</p>
<!-- END_SECTION -->

<h3>Multilevel Cache Example: Quantitative Impact</h3>
<p style="color:whitesmoke;">
Consider a processor with a <strong>base CPI</strong> of 1.0 (<code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">CPU clock rate = 5 GHz</code>) assuming all hits in L1. The <strong>main memory</strong> access time is <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">100 ns</code>, with a <strong>primary (L1) miss rate</strong> of 2% per instruction. If only an L1 cache is present, the <strong>miss penalty</strong> is <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">100ns/0.2ns = 500 cycles</code> (since each clock cycle is 0.2 ns). The <strong>effective CPI</strong> is:
<br>
<code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">
CPI = 1 + 0.02 × 500 = 11
</code>
</p>
<p style="color:whitesmoke;">
Adding a <strong>Level-2 (L2) cache</strong> with <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5 ns</code> access time and enough capacity to reduce the <strong>miss rate to main memory</strong> to 0.5% (global miss rate) leads to the following calculation:
<ul>
  <li><strong>Primary miss with L2 hit:</strong> Penalty = <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5ns/0.2ns = 25 cycles</code></li>
  <li><strong>Primary miss with L2 miss:</strong> Penalty = <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">500 cycles</code></li>
</ul>
<code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">
CPI = 1 + 0.02 × 25 + 0.005 × 500 = 4
</code>
<p style="color:whitesmoke;">The resulting <strong>performance ratio</strong> is <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">11/4 = 2.8</code>, showing a 2.8× speedup over a single-level cache arrangement. This illustrates the considerable performance benefit gained by incorporating an additional cache level, especially when L2 is engineered for low miss rates.</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Multilevel Cache Considerations</strong> affect both L1 and L2 design:
<ul>
  <li><strong>L1 cache</strong> is usually smaller and prioritizes minimal hit time.</li>
  <li><strong>L2 cache</strong> is larger and aims for lower miss rates to avoid costly main memory access, with less stringent hit time requirements.</li>
  <li><strong>L1 block size</strong> is typically smaller than <strong>L2 block size</strong>, balancing fast access and spatial locality benefits.</li>
</ul>
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Summary: Cost/Performance Tradeoffs</strong> center on choosing optimal <strong>memory technologies</strong> across the hierarchy to present the <em>illusion</em> of a large, fast memory. The use of <strong>caches</strong> close to the processor and intelligent organization—such as <strong>direct mapped</strong>, <strong>set associative</strong>, or <strong>fully associative</strong>—is essential. <strong>Write policies</strong> (write-through vs. write-back), <strong>replacement policies</strong> (direct, <strong>LRU</strong>, random), and block size selection all work together to minimize performance loss from cache misses and maximize data throughput.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Tradeoffs to Improve Cache Performance</strong> include using advanced manufacturing technology, faster RAMs, altering <strong>cache size</strong>, block size, or associativity, and adding more cache levels. Reasoned selection among these strategies depends on <strong>cost constraints</strong>, <strong>power budgets</strong>, and <strong>application-specific access patterns</strong>. Generally, decreasing hit time (often by reducing cache size or simplifying organization) lowers the average memory access time, but increasing associativity or cache levels may reduce the miss rate at the cost of higher complexity.
</p>
<!-- END_SECTION -->