<h1>Cache Performance Scaling and Multilevel Cache Architectures</h1>
<p style="color:whitesmoke;">
<strong>Amdahl’s law</strong> is a principle used to evaluate the impact of system improvements, showing that performance gains from one aspect are limited by the time spent on the remaining components. In the context of <strong>cache memory hierarchies</strong>, as processor speed increases while memory latency remains constant, a growing portion of <strong>execution time</strong> is consumed by <strong>memory stalls</strong>. This results in diminishing returns for processor performance scaling, since the fraction of time lost to memory system latencies cannot be reduced solely by making the processor core faster.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Cache Performance Example Revisit:</strong> If the system CPI (cycles per instruction) is decreased from <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">2</code> to <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1</code> without altering the clock rate, using a previous miss penalty analysis of <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">3.44</code> stall cycles per instruction:
</p>
<ol>
  <li>
    <p style="color:whitesmoke;">
      <strong>Calculate new CPI:</strong>
      <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">Actual&nbsp;CPI&nbsp;=&nbsp;1&nbsp;+&nbsp;3.44&nbsp;=&nbsp;4.44</code>
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Ideal CPU speedup:</strong>
      <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">4.44/1&nbsp;=&nbsp;4.44</code> times faster.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Fraction of time in stalls (memory stalls per total cycles):</strong>
      <ul>
        <li>
          <p style="color:whitesmoke;">
            <strong>Before</strong> (CPI = 5.44): <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">3.44/5.44&nbsp;=&nbsp;0.63&nbsp;(63%)</code>
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>After</strong> (CPI = 4.44): <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">3.44/4.44&nbsp;=&nbsp;0.77&nbsp;(77%)</code>
          </p>
        </li>
      </ul>
      As CPI is reduced via processor improvements, the proportion of execution time consumed by memory stalls increases.
    </p>
  </li>
</ol>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Performance Considerations:</strong> Increasing the <strong>clock rate</strong> of a processor without enhancing the memory subsystem results in more CPU cycles being lost to <strong>cache misses</strong>. This is due to the relative increase of <strong>miss penalty</strong> measured in clock cycles.
</p>
<p style="color:whitesmoke;">
<strong>Hit Access Time</strong> is the duration to access data from the cache, including determining whether the access is a hit or miss. If <strong>hit time</strong> increases, so does the <strong>total memory system access time</strong>. <strong>Hit time</strong> is influenced by:
<ul>
  <li>
    <p style="color:whitesmoke;">
      <strong>Cache size:</strong> Larger caches typically increase hit time due to more complex lookup.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Number of pipeline stages:</strong> More stages can reduce hit time but add design complexity.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Cache organization:</strong> More associative caches increase hardware complexity and may increase hit time.
    </p>
  </li>
</ul>
</p>
<!-- END_SECTION -->

<h3>Multilevel Caches</h3>
<p style="color:whitesmoke;">
A <strong>multilevel cache</strong> architecture inserts additional cache levels between the CPU and main memory to reduce the <strong>average memory access time</strong>. The system typically includes:
<ul>
  <li>
    <p style="color:whitesmoke;">
      <strong>Primary (L1) cache:</strong> Directly attached to the CPU, it is very fast but small.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Secondary (L2) cache:</strong> Services all misses from L1. It is larger than L1 but slower, yet much faster than main memory.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Main memory:</strong> Services misses from L2 cache.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>L3 cache:</strong> Some high-end systems employ an additional cache (L3), even larger and slower, to further reduce main memory accesses.
    </p>
  </li>
</ul>
Designers use multilevel caches to achieve low average memory access latency while retaining high cache hit rates.
</p>
<!-- END_SECTION -->

<h3>Multilevel Cache Example</h3>
<p style="color:whitesmoke;">
Given a processor with:
<ul>
  <li>
    <strong>Base CPI</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1.0</code> (if all references hit in the primary cache)</li>
  <li>
    <strong>CPU clock rate</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5~GHz</code></li>
  <li>
    <strong>Main memory access time</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">100~ns</code></li>
  <li>
    <strong>Primary cache miss rate</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">2\%</code></li>
</ul>
The <strong>cycle time</strong> is <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">0.2~ns</code>.
</p>
<ol>
  <li>
    <p style="color:whitesmoke;">
      <strong>Without secondary cache:</strong>
      <ul>
        <li>
          <p style="color:whitesmoke;">
            <strong>Miss penalty</strong> = <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">100\,\text{ns}/0.2\,\text{ns}=500</code> cycles
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>Effective CPI</strong> = <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1+0.02\times500=11</code>
          </p>
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>With a secondary (L2) cache:</strong>
      <ul>
        <li>
          <p style="color:whitesmoke;">
            <strong>L2 access time</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5\,\text{ns}</code> = <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">25</code> cycles
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>Global miss rate to memory</strong> reduced to <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">0.5\%</code>
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>Miss penalty for primary miss with L2 hit</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5\,\text{ns}/0.2\,\text{ns}=25</code> cycles
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>Miss penalty for primary and L2 cache miss</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">500</code> cycles extra
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>CPI calculation:</strong> <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1+0.02\times25+0.005\times500 = 4</code>
          </p>
        </li>
        <li>
          <p style="color:whitesmoke;">
            <strong>Performance ratio:</strong> <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">11/4=2.8</code>
          </p>
        </li>
      </ul>
    </p>
  </li>
</ol>
<p style="color:whitesmoke;">
This example demonstrates the substantial performance gains of adding intermediate cache levels in the hierarchy.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Multilevel Cache Considerations:</strong> 
<ul>
  <li>
    <p style="color:whitesmoke;">
      <strong>Primary (L1) cache:</strong> Prioritizes minimal <strong>hit time</strong> to prevent adding delay to the processor’s critical path.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Secondary (L2) cache:</strong> Emphasizes low <strong>miss rate</strong> rather than hit time, as it exists off the processor’s critical cycle.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Results:</strong> <strong>L1 cache</strong> is usually much smaller than a unified cache would be, and <strong>L1 block size</strong> is typically smaller than <strong>L2 block size</strong>, optimizing for respective goals of low latency and high hit rate.
    </p>
  </li>
</ul>
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Summary of Cache System Design:</strong> 
<ul>
  <li>
    <p style="color:whitesmoke;">
      <strong>Cost/Performance tradeoff:</strong> Different <strong>memory technologies</strong> are balanced in a hierarchy to create the appearance of a large, fast memory.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Cache details:</strong> Caches use <strong>organization</strong> types (direct mapped, set associative, fully associative), <strong>write policies</strong> (write-through, write-back), and <strong>replacement policies</strong> (direct, LRU, random) as key parameters for design and performance.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Design tradeoffs:</strong> Faster memory technologies and smaller caches reduce <strong>hit time</strong>, but may raise <strong>miss rate</strong> or increase cost.
    </p>
  </li>
  <li>
    <p style="color:whitesmoke;">
      <strong>Increasing cache size</strong> and associativity generally lower miss rate, but at the cost of increased hit time and complexity, necessitating evaluation of application requirements and system constraints.
    </p>
  </li>
</ul>
</p>
<!-- END_SECTION -->