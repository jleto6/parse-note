<h1>Cache Memory Organizations, Address Mapping, and Write Policies</h1>
<p style="color:whitesmoke;">
<strong>Cache memory</strong> refers to a small, fast memory component located close to the CPU, used to store copies of frequently accessed main memory data. Its organization and management directly impact processor performance. There are three principal <strong>cache organizations</strong>: <strong>direct mapped</strong>, <strong>set associative</strong>, and <strong>fully associative caches</strong>. Associativity describes the flexibility with which a memory block may be placed in the cache.
</p>
<!-- END_SECTION -->

<h3>Types of Cache Organizations: Direct Mapped, Set Associative, Fully Associative</h3>
<p style="color:whitesmoke;">
In a <strong>direct mapped cache</strong>, each memory block has exactly one location in the cache where it may reside, defined by the formula <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">(block address) mod (#blocks in cache)</code>. This guarantees simple and fast operation but can lead to frequent <strong>conflict misses</strong> if multiple blocks compete for the same position.
</p>
<p style="color:whitesmoke;">
A <strong>fully associative cache</strong> allows any memory block to be stored in any cache block. Searching the entire cache for a block requires a comparator per entry, which increases hardware cost and complexity but nearly eliminates conflict misses.
</p>
<p style="color:whitesmoke;">
A <strong>set associative cache</strong> lies between these extremes. It divides the cache into sets, each containing <strong>n</strong> blocks (for an n-way set associative cache). Each memory block can be placed in any block within one set, which is determined by <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">block number mod (#sets)</code>. When accessing data, all entries within the indexed set are compared in parallel, reducing conflict misses while controlling hardware complexity. Direct mapped and fully associative caches represent 1-way and m-way associativity, respectively, where m is the total number of cache blocks.
</p>
<!-- END_SECTION -->

<h3>Address Subdivision, Tags, Valid Bits, Block Mapping (Indexing and Tag Comparison)</h3>
<p style="color:whitesmoke;">
Each <strong>memory address</strong> is subdivided into fields for use in cache operations:
</p>
<ul>
  <li><strong>Index</strong>: Determines which set or block in the cache to look up.</li>
  <li><strong>Tag</strong>: High-order address bits saved in the cache along with the data to distinguish between different memory blocks mapped to the same cache location or set.</li>
  <li><strong>Valid bit</strong>: Indicates whether the contents of the cache block or set entry are valid and usable. Initially set to 0 (invalid).</li>
</ul>
<p style="color:whitesmoke;">
On a cache access, the <strong>index</strong> field locates the set (for set associative) or block (direct mapped). The <strong>valid bit</strong> is checked first; if set, the <strong>tag</strong> field from the stored entry is compared to the tag portion of the incoming address. If the tags match, this results in a <strong>cache hit</strong>; the data is used directly from the cache. If the valid bit is not set or tags do not match, a <strong>cache miss</strong> occurs and data must be retrieved from a lower memory hierarchy level.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
Below is an example highlighting hit and miss behavior for three organizations (direct mapped, 2-way set associative, and fully associative) on a specific block access sequence.
</p>
<p style="color:whitesmoke;"><strong>Direct mapped, 2-way set associative, fully associative</strong></p>
<p style="color:whitesmoke;">
Block access sequence: 0, 8, 0, 6, 8<br>
Direct mapped: 5 misses
</p>
<table style="color:whitesmoke; border-collapse: collapse;">
<thead>
<tr>
<th style="border: 1px solid #888; padding: 2px 8px;">Block Address</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Cache Index</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Hit/Miss</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Cache Content After Access</th>
</tr>
</thead>
<tbody>
<tr><td style="border: 1px solid #888; padding: 2px 8px;">0</td><td style="border: 1px solid #888; padding: 2px 8px;">0</td><td style="border: 1px solid #888; padding: 2px 8px;">miss</td><td style="border: 1px solid #888; padding: 2px 8px;">Mem[0]</td></tr>
<tr><td>8</td><td>0</td><td>miss</td><td>Mem[8]</td></tr>
<tr><td>0</td><td>0</td><td>miss</td><td>Mem[0]</td></tr>
<tr><td>6</td><td>2</td><td>miss</td><td>Mem[0], Mem[6]</td></tr>
<tr><td>8</td><td>0</td><td>miss</td><td>Mem[8], Mem[6]</td></tr>
</tbody>
</table>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>2-way set associative</strong>
</p>
<table style="color:whitesmoke; border-collapse: collapse;">
<thead>
<tr>
<th style="border: 1px solid #888; padding: 2px 8px;">Block Address</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Cache Index</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Hit/Miss</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Cache Content After Access: Set 0 | Set 1</th>
</tr>
</thead>
<tbody>
<tr><td style="border: 1px solid #888; padding: 2px 8px;">0</td><td style="border: 1px solid #888; padding: 2px 8px;">0</td><td style="border: 1px solid #888; padding: 2px 8px;">miss</td><td style="border: 1px solid #888; padding: 2px 8px;">Mem[0]</td></tr>
<tr><td>8</td><td>0</td><td>miss</td><td>Mem[0], Mem[8]</td></tr>
<tr><td>0</td><td>0</td><td>hit</td><td>Mem[0], Mem[8]</td></tr>
<tr><td>6</td><td>0</td><td>miss</td><td>Mem[0], Mem[6]</td></tr>
<tr><td>8</td><td>0</td><td>miss</td><td>Mem[8], Mem[6]</td></tr>
</tbody>
</table>
<p style="color:whitesmoke;">
This configuration causes 4 misses for this block sequence, demonstrating the partial mitigation of conflict misses compared to direct mapped caches.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Fully associative</strong>
</p>
<table style="color:whitesmoke; border-collapse: collapse;">
<thead>
<tr>
<th style="border: 1px solid #888; padding: 2px 8px;">Block Address</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Hit/Miss</th>
<th style="border: 1px solid #888; padding: 2px 8px;">Cache Content After Access</th>
</tr>
</thead>
<tbody>
<tr><td style="border: 1px solid #888; padding: 2px 8px;">0</td><td style="border: 1px solid #888; padding: 2px 8px;">miss</td><td style="border: 1px solid #888; padding: 2px 8px;">Mem[0]</td></tr>
<tr><td>8</td><td>miss</td><td>Mem[0], Mem[8]</td></tr>
<tr><td>0</td><td>hit</td><td>Mem[0], Mem[8]</td></tr>
<tr><td>6</td><td>miss</td><td>Mem[0], Mem[8], Mem[6]</td></tr>
<tr><td>8</td><td>hit</td><td>Mem[0], Mem[8], Mem[6]</td></tr>
</tbody>
</table>
<p style="color:whitesmoke;">
With maximum associativity, only 3 misses occur, showing the correspondence between associativity and lower miss rates.
</p>
<!-- END_SECTION -->

<p style="color:whitesmoke;">
<strong>Effect of Associativity on Miss Rate</strong>: Increasing associativity reduces miss rate, but the return diminishes with higher associativity. In a system with a 64KB cache and 16-word blocks, simulation results show:
<ul>
  <li>1-way: 10.3%</li>
  <li>2-way: 8.6%</li>
  <li>4-way: 8.3%</li>
  <li>8-way: 8.1%</li>
</ul>
Associativity improves cache utilization but has practical hardware and performance limits.
</p>
<!-- END_SECTION -->

<h3>Cache Replacement Policies</h3>
<p style="color:whitesmoke;">
When a new block must be loaded into a set or cache, the <strong>replacement policy</strong> decides which block to evict if all spots are occupied. In <strong>direct mapped</strong> caches, there is no choice; the specified location is overwritten. In <strong>set associative</strong> or <strong>fully associative</strong> caches, one must choose among blocks in the set:
</p>
<ul>
  <li><strong>Prefer non-valid entry</strong> if one exists (empty slot).</li>
  <li>If all entries are valid, apply a selection policy:
    <ul>
      <li><strong>Least Recently Used (LRU)</strong>: Replace the block that has not been used for the longest time. LRU is easy for 2-way and manageable for 4-way set associative, but impractical for higher associativity due to complexity.</li>
      <li><strong>Random</strong>: Randomly pick a block to replace. For high associativity, this performs similarly to LRU.</li>
    </ul>
  </li>
</ul>
<p style="color:whitesmoke;">
Replacement strategy directly impacts cache hit performance, especially in heavily contended sets or fully associative caches, where the ability to select a block for replacement can minimize cache misses.
</p>
<!-- END_SECTION -->

<h3>Cache Write Policies: Write-Through, Write-Back, Write Buffers, Write Allocation</h3>
<p style="color:whitesmoke;">
When a <strong>store</strong> instruction updates data in the cache, cache coherence with main memory is critical. Two main <strong>write policies</strong> exist:
</p>
<ul>
  <li><strong>Write-Through</strong>: Upon a write hit, data is updated in both the cache and main memory. This simplifies consistency but increases write traffic to main memory, potentially leading to longer stalls. For example, if 10% of instructions are stores and each memory write takes 100 cycles, with a base CPI of 1, the effective CPI may rise to 11. To avoid stalling the CPU whenever a write occurs, a <strong>write buffer</strong> is used, where data awaiting memory write is staged. The CPU only stalls if the buffer is full, otherwise proceeding immediately.</li>
  <li><strong>Write-Back</strong>: On a data-write hit, only the cache is updated. Dirty blocks (blocks modified in cache but not in main memory) are tracked, typically via a <strong>dirty bit</strong>. When a dirty block is evicted, its data is written back to memory, reducing memory traffic but introducing complexity. Write buffers may also be used here to allow reads for replacing blocks to proceed without waiting for immediate writes to memory.</li>
</ul>
<p style="color:whitesmoke;">
<strong>Cache and memory inconsistency</strong> occurs in write-back mode if cache writes are not propagated to main memory. <strong>Write allocation</strong> refers to the policy governing what happens when a write miss occurs; it determines if the block is loaded from memory into the cache (write-allocate) or updated only in main memory (no-write-allocate).
</p>
<!-- END_SECTION -->