**Exceptions and Interrupts**

"Unexpected" events such as exceptions and interrupts require altering control flow. Exceptions occur within the CPU, e.g., undefined opcodes or overflows, while interrupts arise from external I/O controllers. For example, an I/O controller might send an interrupt when a disk finishes reading data, prompting the CPU to switch context to a waiting process.

In MIPS, exceptions are handled by the System Control Coprocessor (CP0), saving the program counter in the Exception Program Counter (EPC), indicating problems in the status register.

The exception handling process involves reading the cause, transferring control to the appropriate handler, and determining required actions. If resolvable, corrective measures are taken; otherwise, the program is terminated, using the EPC to correct errors.

Pipeline exceptions introduce another form of control hazard. For example, overflow during the execution stage requires preventing register overwrite, flushing pipeline instructions, and transferring control to a handler. This approach is similar to handling mispredicted branches, but affects instruction execution in pipelines, simulating exceptions as pauses. Pipelined architecture handles exceptions by treating them similarly to other control flow disruptions.

Challenges arise when instructions depend on accelerator, e.g., cache hazards. Pitfalls in pipelining usually come from memory hazards. Lastly, interrupts can pose challenges for pipelining due to issues like simultaneous memory requests or architectures unexpectedly change registers, a characteristic that depends on memory.
The image contains typed notes on the topic of "Caches" in computing. Here's a transcription:

---

**Caches**

In an ideal computing system, there would be unlimited fast memory access. However, such a system is not practically achievable. Instead, a memory hierarchy provides the illusion of having a large amount of fast memory by organizing different types of storage to optimize speed and access.

The principal of locality refers to the observation that programs tend to access only a small portion of their address space at any given time, which is crucial to optimizing the memory hierarchy. There are two main types of locality:

1. **Temporal locality**: Refers to the tendency of programs to access the same memory locations repeatedly within a short period. For example, instructions in loops and induction variables.
2. **Spatial locality**: Denotes the likelihood of accessing memory locations close to each other during access. This occurs in sequential instruction access and accessing array data.
   
Understanding these principles helps in designing cache systems and hardware to enhance performance.

**Memory Hierarchy Levels**

- Cache: This works like smaller, faster memory that is closer to the processor.
- Data moves through a series of levels of increasing speed and decreasing capacity, from primary storage (like SSD or hard drive) to near memory (DRAM), and then to smaller, faster cache (SRAM) near the CPU.
- Data is moved between levels in blocks, which may contain multiple words. This is a key concept in managing memory transactions.

Caches are used to store frequently accessed data closer to the processor, minimizing access times and improving speed.

Cache memory is the closest level of memory hierarchy to the CPU. It serves to supply either the most frequently or recently accessed data.

A hit occurs when accessed data is present in the upper memory level. A miss occurs when the accessed data is not found and must be fetched from a lower level, resulting in slower access times.

---
