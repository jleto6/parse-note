MEMORY
Memory Hierarchy: Caches

Introduction
• An ideal computing system has unlimited fast memory
• Obviously, this is not possible
• A memory hierarchy gives the illusion of large amounts of fast memory

Principle of Locality
• Programs access only a small proportion of their address space at any time
• Temporal locality
• Items accessed recently are likely to be accessed again soon
• e.g., instructions in a loop, induction variables
• Spatial locality
• Items near those accessed recently are likely to be accessed soon
• E.g., sequential instruction access, array data

Memory Hierarchy
• Smaller, faster memory is closer to the processor

Caches
• Permanent storage on disk
• Copy recently accessed and nearby items from disk to smaller DRAM memory
• Main memory
• Copy recently accessed and nearby items from DRAM to smaller SRAM 
memory
• Cache memory attached to CPU
• Cache may also refer to any storage that takes advantage of locality

Memory Hierarchy Levels
• Block: unit of copying
• May be multiple words
• If accessed data is present in 
upper level
• Hit: access satisfied by upper level
• Hit ratio: hits/accesses
• If accessed data is absent
• Miss: block copied from lower level
• Time taken: miss penalty
• Miss ratio: misses/accesses
= 1 – hit ratio
• Then accessed data supplied from 
upper level

Cache Memory
• Cache memory
• The level of the memory hierarchy closest to the CPU
• Given accesses X1, …, Xn–1, Xn
How do we know if 
the data is present?
Where do we look?

Direct Mapped Cache
• Each word can go in exactly one place in the cache
• Assign a location based on the address of the word
• (Block address) modulo (#Blocks in cache)

Direct Mapped Cache
• Each word can go in exactly one place in the cache
• Assign a location based on the address of the word
• (Block address) modulo (#Blocks in cache)

Direct Mapped Cache
• Each word can go in exactly one place in the cache
• Assign a location based on the address of the word
• (Block address) modulo (#Blocks in cache)
#Blocks is a 
power of 2
Use low-order 
address bits

Tags and Valid Bits
• How do we know which particular block is stored in a cache location?
• Store block address as well as the data
• Only need the high-order bits
• Called the tag
• What if there is no data in a location?
• Valid bit: 1 = present, 0 = not present
• Initially 0

Cache Read
• Reads are simpler because reads do not change the contents of the cache
• Example:
• 8-Block, Direct Mapped Cache
• Addresses:
• Binary
Decimal
Cache Block (address mod 8)
• 10110
22
110
• 11010
26
010
• 10110
22
110
• 11010
26
010
• 10000
16
000
• 00011
3
011
• 10000
16
000
• 10010
18
010
• 10000
16
000

Cache Example
• Initial state
Index
V
Tag
Data
000
N
001
N
010
N
011
N
100
N
101
N
110
N
111
N

Cache Example
Index
V
Tag
Data
000
N
001
N
010
N
011
N
100
N
101
N
110
Y
10
Mem[10110]
111
N
Word addr
Binary addr
Hit/miss
Cache block
22
10 110
Miss
110

Cache Example
Index
V
Tag
Data
000
N
001
N
010
Y
11
Mem[11010]
011
N
100
N
101
N
110
Y
10
Mem[10110]
111
N
Word addr
Binary addr
Hit/miss
Cache block
26
11 010
Miss
010

Cache Example
Index
V
Tag
Data
000
N
001
N
010
Y
11
Mem[11010]
011
N
100
N
101
N
110
Y
10
Mem[10110]
111
N
Word addr
Binary addr
Hit/miss
Cache block
22
10 110
Hit
110
26
11 010
Hit
010

Cache Example
Index
V
Tag
Data
000
Y
10
Mem[10000]
001
N
010
Y
11
Mem[11010]
011
Y
00
Mem[00011]
100
N
101
N
110
Y
10
Mem[10110]
111
N
Word addr
Binary addr
Hit/miss
Cache block
16
10 000
Miss
000
3
00 011
Miss
011
16
10 000
Hit
000

Cache Example
Index
V
Tag
Data
000
Y
10
Mem[10000]
001
N
010
Y
10
Mem[10010]
011
Y
00
Mem[00011]
100
N
101
N
110
Y
10
Mem[10110]
111
N
Word addr
Binary addr
Hit/miss
Cache block
18
10 010
Miss
010

Address Subdivision

Cache Hit
• Index of the address specifies cache block
• If the valid bit indicates data is present, check the tags
• If the tags match, it is a hit
• We can use the data stored in the cache 
• CPU proceeds normally

Cache Misses
• If the valid bit is not set or if the tags do not match, it is a miss
• Control unit must detect and process a miss
• Creates a stall in the pipeline

Cache Miss (Instruction Memory)
1. Send the original PC value (current PC – 4) to the memory.
2. Instruct main memory to perform a read and wait for the memory to 
complete its access.
3. Write the cache entry, putting the data from memory in the data portion of 
the entry, writing the upper bits of the address (from the ALU) into the tag 
field, and turning the valid bit on.
4. Restart the instruction execution at the first step, which will refetch the 
instruction, this time finding it in the cache.

Cache Miss
• Stall until the data is fetched from memory
• Instruction cache miss
• Restart instruction fetch
• Data cache miss
• Complete data access

Cache Miss
• Misses are classified into three different types
• Compulsory (cold start)
• First access to a piece of data
• Cannot be avoided
• Capacity
• Working set of program is larger than cache size
• Not enough room to cache a locality
• Conflict
• collisions, multiple blocks competing for the same space
• Misses may be reduced by changing the associativity of the cache

Cache Organizations
• Direct mapped
• Set associative
• Fully associative

Fully Associative
• A block in memory may be associated with any entry in the cache
• Requires all entries to be searched at once
• Comparator per entry (expensive)

N-way Set Associative
• Each block may go to n locations
• Each set contains n entries
• Block number determines which set
• (Block number) modulo (#Sets in cache)
• Search all entries in a given set at once
• n comparators (less expensive)

Associative Cache Example

Associativity
• All cache organizations are a variation of set associativity.
• Direct mapped: 1-way set associative
• Fully associative: m-way associative where m is the number of blocks in the 
cache

Spectrum of Associativity
• For a cache with 8 entries

Set Associative Cache Organization

Replacement Policy
• Direct mapped: no choice
• Set associative
• Prefer non-valid entry, if there is one
• Otherwise, choose among entries in the set
• Least-recently used (LRU)
• Choose the one unused for the longest time
• Simple for 2-way, manageable for 4-way, too hard beyond that
• Random
• Gives approximately the same performance as LRU for high associativity

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]
8
0
miss
Mem[8]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]
8
0
miss
Mem[8]
0
0
miss
Mem[0]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]
8
0
miss
Mem[8]
0
0
miss
Mem[0]
6
2
miss
Mem[0]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]
8
0
miss
Mem[8]
0
0
miss
Mem[0]
6
2
miss
Mem[0]
Mem[6]
8
0
miss
Mem[8]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Direct mapped
• 5 misses
Block 
address
Cache 
index
Hit/miss
Cache content after access
0
1
2
3
0
0
miss
Mem[0]
8
0
miss
Mem[8]
0
0
miss
Mem[0]
6
2
miss
Mem[0]
Mem[6]
8
0
miss
Mem[8]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]
8
0
miss
Mem[0]
Mem[8]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]
8
0
miss
Mem[0]
Mem[8]
0
0
hit
Mem[0]
Mem[8]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]
8
0
miss
Mem[0]
Mem[8]
0
0
hit
Mem[0]
Mem[8]
6
0
miss
Mem[0]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]
8
0
miss
Mem[0]
Mem[8]
0
0
hit
Mem[0]
Mem[8]
6
0
miss
Mem[0]
Mem[6]
8
0
miss
Mem[8]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• 2-way set associative
• 4 misses
Block 
address
Cache 
index
Hit/miss
Cache content after access
Set 0
Set 1
0
0
miss
Mem[0]
8
0
miss
Mem[0]
Mem[8]
0
0
hit
Mem[0]
Mem[8]
6
0
miss
Mem[0]
Mem[6]
8
0
miss
Mem[8]
Mem[6]

Associativity Example
• Compare 4-block caches
• Direct mapped, 2-way set associative, fully associative
• Block access sequence: 0, 8, 0, 6, 8
• Fully associative
• 3 misses
Block 
address
Hit/miss
Cache content after access
0
miss
Mem[0]
8
miss
Mem[0]
Mem[8]
0
hit
Mem[0]
Mem[8]
6
miss
Mem[0]
Mem[8]
Mem[6]
8
hit
Mem[0]
Mem[8]
Mem[6]

How Much Associativity
• Increased associativity decreases miss rate
• But with diminishing returns
• Simulation of a system with 64KB cache, 16-word blocks
• 1-way: 10.3%
• 2-way: 8.6%
• 4-way: 8.3%
• 8-way: 8.1%

Cache Write
• If we replace the data memory in our datapath with a cache, what happens on 
a store word instruction?
• If we wrote the data into only the data cache without changing main memory; 
then main memory would have a different value from that in the cache. 
• In such a case, the cache and memory are said to be inconsistent.

Write-Through
• Whenever changes are made to the data in the cache, also change 
the data in main memory.

Write-Through
• Whenever changes are made to the data in the cache, also change 
the data in main memory.
• Writes take longer
• Suppose 10% of instructions are stores and write to memory takes 100 
cycles
• If base CPI = 1, then effective CPI = 1 + 0.1×100 = 11

Write-Through
• Whenever changes are made to the data in the cache, also change 
the data in main memory.
• Writes take longer
• Solution: write buffer
• Holds data waiting to be written to memory
• CPU continues immediately
• Only stalls on write if write buffer is already full

Write-Back
• On data-write hit, just update the block in cache
• Keep track of whether each block is “clean” or “dirty”
• When a dirty block is replaced
• Write it back to memory
• Can use a write buffer to allow replacing block to be read first

Write Allocation
• What should happen on a write miss?
• Alternatives for write-through
• Allocate on miss: fetch the block
• Write around: don’t fetch the block
• Since programs often write a whole block before reading it (e.g., initialization)
• For write-back
• Usually fetch the block

Performance
• The time to service hits and misses affects the CPU time.
• Hit time
• time required to access the cache
• includes time to determine if it’s a hit or a miss
• Miss penalty
• time required to fetch a block from the next lowest level
• Hit time < Miss penalty

Measuring Cache Performance
• Components of CPU time
• Program execution cycles
• Includes cache hit time
• Memory stall cycles
• Mainly from cache misses
• CPU time = (CPU cycles + Memory-stall cycles) * Clock cycle time
• Memory-stall cycles = Read-stall cycles + Write-stall cycles
• Read-stall cycles = Reads/Program * Read miss rate * Read miss penalty
• Write-stall cycles = (Writes/Program * write miss rate * write miss penalty) + Write buffer stalls

Measuring Cache Performance
• Components of CPU time
• Program execution cycles
• Includes cache hit time
• Memory stall cycles
• Mainly from cache misses
penalty
 
Miss
n
Instructio
Misses
Program
ns
Instructio
penalty
 
Miss
rate
 
Miss
Program
accesses
Memory 
cycles
 
stall
Memory 
  


=


=

Cache Performance Example
• Assume an instruction cache miss rate for a program is 2% and a data cache 
miss rate is 4%. If a processor has a CPI of 2 without any memory stalls and 
the miss penalty is 100 cycles for all misses, determine how much faster a 
processor would run with a perfect cache that never missed.  Assume the 
frequency of all loads and stores is 36%.

Cache Performance Example
• Given
• I-cache miss rate = 2%
• D-cache miss rate = 4%
• Base CPI (ideal cache) = 2
• Miss penalty = 100 cycles
• Load & stores are 36% of instructions

Cache Performance Example
• Given
• I-cache miss rate = 2%
• D-cache miss rate = 4%
• Base CPI (ideal cache) = 2
• Miss penalty = 100 cycles
• Load & stores are 36% of instructions
• Determine miss cycles per instruction
• I-cache: 
0.02 × 100 = 2
• D-cache: 
0.36 × 0.04 × 100 = 1.44
• Memory miss cycles = 2 + 1.44 = 3.44

Cache Performance Example
• Given
• I-cache miss rate = 2%
• D-cache miss rate = 4%
• Base CPI (ideal cache) = 2
• Miss penalty = 100 cycles
• Load & stores are 36% of instructions
• Determine miss cycles per instruction
• I-cache: 
0.02 × 100 = 2
• D-cache: 
0.36 × 0.04 × 100 = 1.44
• Memory miss cycles = 2 + 1.44 = 3.44
• Actual CPI = 2 + 3.44 = 5.44

Cache Performance Example
• Given
• I-cache miss rate = 2%
• D-cache miss rate = 4%
• Base CPI (ideal cache) = 2
• Miss penalty = 100 cycles
• Load & stores are 36% of instructions
• Determine miss cycles per instruction
• I-cache: 
0.02 × 100 = 2
• D-cache: 
0.36 × 0.04 × 100 = 1.44
• Memory miss cycles = 2 + 1.44 = 3.44
• Actual CPI = 2 + 3.44 = 5.44
• Ideal CPU is 5.44/2 =2.72 times faster

Performance Considerations
• What happens if the processor is made faster, but the memory system is not?  
• The amount of time spent on memory stalls will take up an increasing fraction of the 
execution time
• Amdahl’s law

Cache Performance Example Revisit
• Suppose we speed up the system by reducing its CPI from 2 to 1 without 
changing the clock rate.  
• Actual CPI = 1 + 3.44 = 4.44
• Ideal CPU is 4.44/1 = 4.44 times faster
• Amount of time spent on memory stalls increases:
• Before: 
3.44/5.44 = 63%
• Now:
3.44/4.44 = 77%

Performance Considerations
• Increasing clock rate without changing the memory system also increases the 
performance lost due to cache misses. 
• Memory stalls account for more CPU cycles

Performance Considerations
• Hit Access Time
• If the hit time increases, the total time to access a word from the memory system will 
increase.
• Affect by:
• Cache size
• Number of pipeline stages
• Cache organization

Multilevel Caches
• Primary cache attached to CPU
• Small, but fast
• Level-2 cache services misses from primary cache
• Larger, slower, but still faster than main memory
• Main memory services L-2 cache misses
• Some high-end systems include L-3 cache

Multilevel Cache Example
• Suppose we have a processor with a base CPI of 1.0, assuming all references 
hit in the primary cache, and a clock rate of 5 GHz. Assume a main memory 
access time of 100 ns, including all the miss handling. Suppose the miss rate 
per instruction at the primary cache is 2%. 
• How much faster will the processor be if we add a secondary cache that has a 
5 ns access time for either a hit or a miss and is large enough to reduce the 
miss rate to main memory to 0.5%?

Multilevel Cache Example
• Given
• CPU base CPI = 1
• CPU clock rate = 5 GHz
• Miss rate/instruction = 2%
• Main memory access time = 100ns

Multilevel Cache Example
• Given
• CPU base CPI = 1
• CPU clock rate = 5 GHz
• Miss rate/instruction = 2%
• Main memory access time = 100ns
• With just primary cache
• Miss penalty = 100ns/0.2ns = 500 cycles
• Effective CPI = 1 + 0.02 × 500 = 11

Multilevel Cache Example
• Now add L-2 cache
• Access time = 5ns
• Global miss rate to main memory = 0.5%
• Primary miss with L-2 hit
• Penalty = 5ns/0.2ns = 25 cycles
• Primary miss with L-2 miss
• Extra penalty = 500 cycles
• CPI = 1 + 0.02 × 25 + 0.005 × 500 =4
• Performance ratio = 11/4 = 2.8

Multilevel Cache Considerations
• Primary cache
• Focus on minimal hit time
• L-2 cache
• Focus on low miss rate to avoid main memory access
• Hit time has less overall impact
• Results
• L-1 cache usually smaller than a single cache
• L-1 block size smaller than L-2 block size

Summary
• Cost/Performance tradeoff in memory technologies
• Create an illusion of large amounts of fast memory with a hierarchy
• Uses caches close to the processor
• Cache Details
• Organization: direct mapped, set associative, fully associative
• Write policy: write-through vs. write-back
• Replacement policy: direct, LRU, random

Summary
• Tradeoffs to Improve Cache Performance:
• Use better technology
• Use faster RAMs
• Cost and availability are limitations
• Decrease Hit Time
• Make cache smaller, but miss rate increases
• Use direct mapped, but miss rate increases
• Decrease Miss Rate
• Make cache larger, but can increases hit time
• Add associativity, but can increase hit time
• Increase block size, but increases miss penalty
• Decrease Miss Penalty
• Reduce transfer time component of miss penalty
• Add another level of cache

The image is a diagram labeled "Datapath." Here's an analysis of it:

### Layout Description:
- **Main Components:** 
  - **Instruction Memory:** Positioned on the left, labeled "Instruction memory."
  - **Registers Block:** In the center, showing registers for storing data.
  - **ALU (Arithmetic Logic Unit):** Central component performing operations.
  - **Data Memory:** Placed on the right for data storage operations.
  - **Multiplexers (MUX):** Multiple instances are labeled "MUX" to select data paths.

### Detailed Elements:
- **Instruction Path:** Begins with reading the address from the "PC" (Program Counter). The address is directed to the "Instruction memory."
- **Controllers and Data Flow:**
  - "Read address" to "Instruction memory" to get instructions.
  - "Registers" block shows "Register 1" and "Register 2" for inputs, with an option to write data.
  - "Sign-extend" block is used to handle immediate values.
- **ALU Path:** 
  - Receives inputs from "Register 1" and the result of a "MUX" that selects either "Register 2" data or a "Sign-extend" output.
  - ALU performs operations defined by "ALU operation."
- **Data Memory:**
  - Data from ALU or to be written/read is processed through "Data memory," directed by "MemWrite" and "MemRead" controls.
- **Controls and Multiplexers:**
  - "MUX" instances at various points to control the flow using signals like "PCSrc," "ALUSrc," and paths for ALU results or memory data.

### Insights:
- **Sequential Process:** The datapath is designed to facilitate the flow of instructions and data through the CPU.
- **Control Signals:** Various control signals manage the reading, writing, and operation commands.
- **ALU Operations:** Highlights how different paths are selected for execution based on instructions and control inputs.
- **Memory Interaction:** Data and instructions are read from separate memories and controlled precisely through multiplexers.

This diagram represents a typical CPU datapath used in computer architecture to process instructions and handle data manipulation effectively.
PROCESSOR 
DATAPATH

Introduction
• MIPS ISA
• 3 Instruction Types
• R-type, I-type, J-type
• Datapath and Control Unit
• Simplified
• Pipelined
• Simple subset
• Memory reference: lw, sw
• Arithmetic/logical: 
add, addi, sub, and, or, slt
• Control transfer: 
beq, j

Instruction Format Review
• Three  instruction formats
• R-type
• I-type
• J-type
• Fields:
• op: operation of the instruction
• rs, rt, rd: source/destination register specifiers
• shamt: shift amount
• funct: selects variant of the operation in the “op” field
• address/immediate: address offset or immediate value
• target address: target address of the jump instruction 
op
target address
0
26
31
6 bits
26 bits
op
rs
rt
rd
shamt
funct
0
6
11
16
21
26
31
6 bits
6 bits
5 bits
5 bits
5 bits
5 bits
op
rs
rt
immediate
0
16
21
26
31
6 bits
16 bits
5 bits
5 bits

MIPS Subset
• Add, Subtract, AND, OR, SLT
• add rd, rs, rt
• sub rd, rs, rt
• and rd, rs, rt
• or rd, rs, rt
• slt rd, rs, rt
• ADD Immediate
• addi  rt, rs, imm16
• Load, Store
• lw rt, rs, imm16
• sw rt, rs, imm16
• BRANCH
• beq rs, rt, imm16
• JUMP:
• j  target
op
rs
rt
rd
shamt
funct
0
6
11
16
21
26
31
6 bits
6 bits
5 bits
5 bits
5 bits
5 bits
op
rs
rt
immediate
0
16
21
26
31
6 bits
16 bits
5 bits
5 bits
op
target address
0
26
31
6 bits
26 bits

Instruction Execution
• For every instruction:
1.
Send the program counter (PC) to the memory that contains the 
code and fetch the instruction from that memory.
1.
Read one or two registers, using fields of the instruction to select 
the registers to read. For the load word instruction, we need to 
read only one register, but most other instructions require that we 
read two registers.

Instruction Execution
• Fetch:
• Get the next instruction: stored at PC

Instruction Execution
• Decode:
• Opcode determines if the instruction is arithmetic/logical, memory 
logical, or branch.

Instruction Execution
• Update PC:  PC target address or PC + 4
• If the instruction is a branch, we have to calculate the target 
address
• If the instruction is not a branch, the new PC will be PC + 4

Instruction Execution
• Update PC:  PC target address or PC + 4
• If the instruction is a branch, we have to calculate the target 
address
• If the instruction is not a branch, the new PC will be PC + 4

Instruction Execution
• Execute:
• Depending on instruction class, we use the ALU to calculate:
• Arithmetic result
• Memory address for load/store
• Branch target address

Instruction Execution
• To read and write registers, we need a register file
• R-types read two registers and write one register
• Memory-reference instructions read one register
• Also access data memory

Processor Overview
Data “flows” through 
memory and 
functional units

Processor Overview
Can’t just join 
wires together
Use multiplexers

Processor Overview
Multiplexers require 
selection signals 
Control Signals

Signals
• Information encoded in binary
• Low voltage = 0, High voltage = 1
• One wire per bit
• Multi-bit data encoded on multi-wire buses

Logic Design Review
• Combinational Circuits
• Output depends only on current input
• Operate on data
• Example: ALU
• Given the same input a combinational
circuit will always produce the same 
output

Logic Design Review
• Sequential Circuits
• Have “state”
• Output depends on current input and previous outputs
• Example: Register File
• Output depends on the contents
of the registers in the file

MIPS Register File
• 32 32-bit registers
• One write bus
• Two read buses
• Selection Inputs
• RegWrite (write enable signal) 
• Reg. Source A
• Reg. Source B
• Reg. Destination

Write Control Signals
Every Clock Cycle
• We do not show a write control 
signal when a state element is 
written on every active clock 
edge.
• Program Counter
When Necessary
• If a state element is not 
updated on every clock, then 
an explicit write control signal is 
required.
• Register File

Datapath Elements
• Most elements have 32-bit wide inputs and outputs 
• Buses labeled with their width

Datapath Elements - Combinational
I0
I1
Y
M
u
x
S
Multiplexer
Y = S ? I1 : I0
A
B
Y
+
A
B
Y
ALU
F
Adder
Y = A + B
Arithmetic/Logic Unit
Y = F(A, B)

Datapath Elements - Sequential

Fetch Elements
• Memory Unit
• Program Counter
• Adder

R-Type Instructions
• Read two register operands
• Perform arithmetic/logical operation
• Write register result

R-Type Instructions
add $t1, $t2, $t3
• RW = 9
• RA = 10
• RB = 11
• ALUop = “add”

Datapath: R-Type Instructions

I-Type Instructions
• Replace one read register, shamt, and funct with 16 bit 
constant
• ALU requires 32-bit inputs
• Sign-extend the 16 bit immediate
• Fill with 0s if the constant is positive
• Fill with 1s if the constant is negative
• Choose between the second read registers 
and the sign extended constant.

Datapath: I-Type Instructions

Load/Store Instructions
• lw 
$t1, offset($t2)
• sw 
$t1, offset($t2)
• Compute memory address: $t2 + offset
• Since offset is 16 bits, it needs to be extended to 32 bits

Load/Store Instructions
• Load: Read memory and update register
• Store: Write register value to memory
• Elements: 
• Register file
• Sign extension unit
• Data memory unit

Datapath: Load Instruction

Datapath: Store Instruction

Branch Instructions
beq
$t1, $t2, offset
• Read register operands
• Compare operands
• Use ALU, subtract and check Zero output
• Calculate target address
• Sign-extend displacement
• Shift left 2 places (word displacement)
• Add to PC + 4
• Already calculated by instruction fetch

Target Address Details
• The base for the branch address calculation is the 
address of the instruction following the branch. 
• PC+4
• Memory is byte addressed 
• The offset field must be shifted left 2 bits

Branch Instructions
Just
re-routes 
wires
Sign-bit wire 
replicated

Branching
beq
$t1, $t2, offset
• Branch is taken
• When $t1 - $t2 = 0, the zero signal from the ALU
• PC = PC + 4 + 4*offset
• Branch is not taken
• PC = PC + 4

Datapath: Branch

Jump Instructions
• PC = PC[31-28] : Offset << 2

Single Datapath
• All instructions executed in one clock cycle
• Each datapath element can only do one function at a time
• Any element needed more than once must be duplicated
• Hence, we need separate instruction and data memories
• Use multiplexers where alternate data sources are used 
for different instructions

Control Unit
• Take in input
• Generate signals for each state element
• Generate selection signals for each multiplexor
• Generate function signals for ALU
• ALU Control

ALU Control
• MIPS subset: lw, sw, beq, add, addi, sub, and, or, slt
• ALU has 4 control inputs
• 16 possible functions
ALU Control
Function
0000
AND
0001
OR
0010
Add
0110
Subtract
0111
Set-on-less-than
1100
NOR

ALU Control
• Load/Store: add
• Branch: subtract
• R-type: depends on funct field
ALU Control
Function
0000
AND
0001
OR
0010
Add
0110
Subtract
0111
Set-on-less-than
1100
NOR

ALU Control
• Opcode determines which type of instruction
• The ALUOp is a 2-bit signal derived from this Opcode
• ALUOp and the funct field will determine ALU control
Opcode
ALUOp
Operation
Funct
ALU function
ALU Control
Lw
00
Load word
XXXXXX
Add
0010
Sw
00
Store word
XXXXXX
Add
0010
Beq
01
Branch
XXXXXX
Subtract
0110
R-type
10
Add
100000
Add
0010
Subtract
100010
Subtract
0110
AND
100100
AND
0000
OR
100101
OR
0001
Set-on-Less-
Than
101010
Set-on-Less-
Than
0111

ALU Control
• Multiple levels of decoding:
• Main Control generates ALUOp
• ALUOp and funct bits determine ALU Control

ALU Control
• ALUOp and funct bits determine ALU Control
ALUOp1
ALUOp2
Funct
Operation
0
0
X
X
X
X
X
X
0010 (add)
X
1
X
X
X
X
X
X
0110 (subtract)
1
X
X
X
0
0
0
0
0010 (add)
1
X
X
X
0
0
1
0
0110 (subtract)
1
X
X
X
0
1
0
0
0000 (and)
1
X
X
X
0
1
0
1
0001 (or)
1
X
X
X
1
0
1
0
0111 (slt)

The Main Control Unit
• Control signals derived from instruction
0
rs
rt
rd
shamt
funct
31:26
5:0
25:21
20:16
15:11
10:6
35 or 43
rs
rt
address
31:26
25:21
20:16
15:0
4
rs
rt
address
31:26
25:21
20:16
15:0
R-type
Load/
Store
Branch
opcode
always 
read
read, 
except 
for load
write for 
R-type 
and load
sign-extend 
and add

Main Control Unit
• Observations
• The opcode is always contained in bits 31:26. We will refer to this 
field as Op[5:0].
• The two registers to be read are always specified by the rs and rt 
fields, at positions 25:21 and 20:16. This is true for the R-type 
instructions, branch equal, and for store. 
• The base register for load and store instructions is always in bit 
positions 25:21 (rs). 
• The 16-bit offset for branch equal, load, and store is always in 
positions 15:0. 
• The destination register is in one of two places. 
• For a load it is in bit positions 20:16 (rt)
• For an R-type instruction it is in bit positions 15:11 (rd)
• We will need to add a multiplexor to select which field of the instruction is used 
to indicate the register number to be written.

Datapath with ALU Control

Control Signals – Register Destination
• RegDst
• Deasserted: the register destination number for the write register 
comes from the rt field (bits 20:16)
• Asserted: the register destination number for the write register 
comes from the rd field (bits 15:11)

Control Signals – Register Write
• RegWrite
• Asserted: the register destination specified by the write register 
input is written with the value from the write data input

Control Signals – ALU Source
• ALUSrc
• Deasserted: The second ALU operand comes from the second 
register file output
• Asserted: The second ALU operand is the sign-extended lower 16 
bits of the instruction

Control Signals - PCSrc
• Program Counter Source
• Deasserted: The PC is replaced by PC + 4
• Asserted: The PC is replaced by a branch target address

Control Signals – Memory Read
• MemRead
• Asserted: Data memory contents designated by the address input 
are put on the read data output.

Control Signals – Memory Write
• MemWrite
• Asserted: Data memory contents designated by the address input 
are replaced by the value on the write data input.

Control Signals – Memory to Register
• MemtoReg
• Deasserted: The value fed to the register write data input comes 
from the ALU
• Asserted: The value fed to the register write data input comes from 
the data memory.

Control Signals
• All but one signal can be set based only on the opcode
• PCSrc is the exception
• Relies on the result of a branch
• PCSrc should be set if the instruction is beq and the zero output of 
the ALU is asserted

Datapath With Control

Opcode to Control
Instruction
RegDst
ALUSrc
MemtoReg
RegWrite
R-Format
1
0
0
1
Lw
0
1
1
1
Sw
X
1
X
0
Beq
X
0
X
0
Instruction
MemRead
MemWrite
Branch
ALUOp1
ALUOp0
R-Format
0
0
0
1
0
Lw
1
0
0
0
0
Sw
0
1
0
0
0
Beq
0
0
1
0
1

R-Type Instruction

Load Instruction

Branch-on-Equal Instruction

Opcode to Control
In/Out 
Signal
R-Format
Lw
Sw
Beq
Inputs
Op5
0
1
1
0
Op4
0
0
0
0
Op3
0
0
1
0
Op2
0
0
0
1
Op1
0
1
1
0
Op0
0
1
1
0
Outputs
RegDst
1
0
X
X
ALUSrc
0
1
1
0
MemToReg
0
1
X
X
RegWrite
1
1
0
0
MemRead
0
1
0
0
MemWrite
0
0
1
0
Branch
0
0
0
1
ALUOp1
1
0
0
0
ALUOp0
0
0
0
1

Implementing Jumps
• Jump uses word address
• Update PC with concatenation of
• Top 4 bits of old PC
• 26-bit jump address
• 00
• Need an extra control signal decoded from opcode
2
address
31:26
25:0
Jump

Datapath With Jumps Added

Single-Cycle Implementation
• Every instruction begins execution on one clock edge and 
completes execution on the next clock edge.
• Clock cycle time must be at least as long as the longest 
instruction (load word).
• Cycle time = 
PC’s propagation time + 
Instruction Memory Access Time +
Register File Access Time  +
ALU Delay (address calculation)  +
Data Memory Access Time  +
Register File Setup Time  +
Clock Skew

Single-Cycle Implementation
• Every instruction begins execution on one clock edge and 
completes execution on the next clock edge.
• Clock cycle time must be at least as long as the longest 
instruction (load word).
• The cycle time for load is much longer than any other instruction

Single-Cycle Implementation
• Single-Cycle implementation is not practical
• Unable to implement more complex instructions
• Cannot make improves that will speed up the system unless those 
improvements are to load
• Violates the design principle of making the common case fast
• Some functional units must be duplicated, increasing hardware cost

Pipelining
• Pipelining is an implementation technique in which 
multiple instructions are overlapped in execution.  
• Pipelining is nearly universal.

Analogy for Pipelining: Laundry
• Nonpipelined approach:
1.
Place one dirty load of clothes in the washer.
2.
When the washer is finished, place the wet load in the dryer.
3.
When the dryer is finished, place the dry load on a table and fold.
4.
When folding is finished, put the clothes away.
• When the clothes are put away, the next load can begin.

Analogy for Pipelining: Laundry
• Pipelined approach:
1.
Place one dirty load (load A) of clothes in the washer. 
2.
When the washer is finished, place load A in the dryer and start a 
new load (load B) in the washer.
3.
When the dryer is finished, place load A on the table to fold, 
place load B in the dryer, and start a new load (load C) in the 
washer.
4.
When folding is finished, put away load A, fold load B, place load 
C in the dryer, start load D in the washer.

Analogy for Pipelining: Laundry

Pipelining Paradox
• One load of laundry still takes the same amount of time.
• The amount of time it takes to do many loads of laundry is 
shorter with pipelining.
• Pipelining improves throughput.

Pipelining Speedup
• If all the stages take about the same amount of time and 
there is enough work to do, then the speedup due to 
pipelining is roughly equal to the number of stages in the 
pipeline.
• Laundry Analogy: 
• 4 stages (washing, drying, folding, putting away)
• 20 loads pipelined would take about 5 times as long as 1 load
• 20 loads of sequential laundry takes 20 times as long as 1 load

MIPS Pipeline
1.
Fetch instruction from 
memory.
1.
Read registers while 
decoding the instruction. 
1.
Execute the operation or 
calculate an address.
1.
Access an operand in data 
memory.
1.
Write the result into a register.
• Instruction Fetch
(IF)
• Instruction Decode
(ID)
• Execution
(EX)
• Memory Access
(MEM)
• Write Back
(WB)

Pipeline Speedup
• If all stages are balanced
• i.e., all take the same time
• Time between instructionspipelined
=
Time between instructionsnonpipelined
Number of stages
• If not balanced, speedup is less
• Speedup due to increased throughput
• Latency (time for each instruction) does not decrease
• Programs execute billions of instructions – throughput is important!

Pipelining and ISA Design
• MIPS ISA designed for pipelining
• All instructions are 32-bits
• Easier to fetch and decode
• Few and regular instruction formats
• Can decode and read registers in one step
• Load/store addressing
• Can calculate address in 3rd stage, access memory in 4th stage
• Alignment of memory operands
• Memory access takes only one cycle

MIPS Pipelined Datapath
WB
Branch

Pipelined Execution

Pipeline registers
• Need registers between stages
• To hold information produced in previous cycle

IF for Load

ID for Load

EX for Load

MEM for Load

WB for Load
Wrong
register
number

Corrected Datapath for Load

Pipelined Control

Pipelined Control
•
Instruction fetch: 
•
PC written on every clock cycle
•
Instruction memory read on every clock cycle
•
no optional control lines
•
Instruction decode/register file read
•
Register file read on every clock cycle
•
no optional control lines
•
Execution:
•
Set RegDst:
select the Result register
•
Set ALUOp:
select the ALU operation
•
Set ALUSrc. 
select either Read data 2 or a sign-extended immediate for the ALU.
•
Memory access: 
•
Set Branch:
affects PCSrc
•
Set MemRead
•
Set MemWrite. 
•
Write back: 
•
Set MemtoReg:
decides between sending the ALU result or the memory value to the register file
•
Set Reg-Write:
specifies whether the register file can be written

Pipelined Control
RegDst
ALUSrc
ALUOp1
ALUOp0
Branch
MemRead
MemWrite
MemtoReg
Regwrite

Pipelined Control

Hazards
• Situations that prevent starting the next instruction in the 
next cycle
• Structure hazard
• A required resource is busy
• Data hazard
• Need to wait for previous instruction to complete its data read/write
• Control hazard
• Deciding on control action depends on previous instruction

Structure Hazards
• Conflict for use of a resource
• In MIPS pipeline with a single memory
• Load/store requires data access
• Instruction fetch would have to stall for that cycle
• Would cause a pipeline “bubble”
• Hence, pipelined datapaths require separate 
instruction/data memories
• Or separate instruction/data caches

Data Hazards
• An instruction depends on completion of data access by a 
previous instruction
• add$s0, $t0, $t1
• sub$t2, $s0, $t3

Forwarding (aka Bypassing)
• Use result when it is computed
• Don’t wait for it to be stored in a register
• Requires extra connections in the datapath

Forwarding
• Forwarding: the result is passed forward from an earlier 
instruction to a later instruction.

Load-Use Data Hazard
• Can’t always avoid stalls by forwarding

Pipeline Stalls
• A stall (bubble) is implemented with a nop
• “No operation”
• Force control values in ID/EX register to 0

Stall/Bubble in the Pipeline
Stall inserted 
here

Another view (no bubble)...
Stall inserted 
here

Another view (with bubble)...
Stall inserted 
here

Code Scheduling to Avoid Stalls
• Reorder code to avoid use of load result in the next 
instruction
• C code for A = B + E; C = B + F;
lw
$t1, 0($t0)
lw
$t2, 4($t0)
add $t3, $t1, $t2
sw
$t3, 12($t0)
lw
$t4, 8($t0)
add $t5, $t1, $t4
sw
$t5, 16($t0)
stall
stall
lw
$t1, 0($t0)
lw
$t2, 4($t0)
lw
$t4, 8($t0)
add $t3, $t1, $t2
sw
$t3, 12($t0)
add $t5, $t1, $t4
sw
$t5, 16($t0)
11 cycles
13 cycles

Control Hazards
• Branch Hazards
• Branch determines flow of control
• Fetching next instruction depends on branch outcome
• Pipeline can’t always fetch correct instruction
• Still working on ID stage of branch

Stall on Branch
• Wait until branch outcome determined before fetching 
next instruction

Branch Prediction
• Longer pipelines can’t readily determine branch outcome 
early
• Stall penalty becomes unacceptable
• Predict outcome of branch
• Only stall if prediction is wrong
• In MIPS pipeline
• Can predict branches not taken
• Fetch instruction after branch, with no delay

Predict Branch Not Taken
• If branch is determined as taken in MEM:
PC
Flush these
instructions
(Set control
values to 0)

Predict Branch Not Taken
Prediction 
correct
Prediction 
incorrect

Predict Branch Not Taken
• If branches are untaken half the time, and if it costs little to 
discard the instructions, this optimization halves the cost 
of control hazards.

Branch Prediction
• Static branch prediction
• Based on typical branch behavior
• Example: loop and if-statement branches
• Predict backward branches taken
• Predict forward branches not taken

Dynamic Branch Prediction
• In deeper and superscalar pipelines, branch 
penalty is more significant
• Use dynamic prediction
• Branch History Table
• Indexed by recent branch instruction addresses
• Stores outcome (taken/not taken)
• To execute a branch
• Check table, expect the same outcome
• Start fetching from fall-through or target
• If wrong, flush pipeline and flip prediction

1-Bit Predictor: Shortcoming
• Inner loop branches mispredicted twice!
outer: …
…
inner: …
…
beq …, …, inner
…
beq …, …, outer
Mispredict as taken on last iteration of 
inner loop
Then mispredict as not taken on first 
iteration of inner loop next time around

2-Bit Predictor
• Only change prediction on two successive mispredictions

Delayed Decision
• Branch Instruction
• Execute the next sequential instruction
• Execute the instruction that resulted from the branch
• add $4, $5, $6
->
beq $1, $2, 40
• beq $1, $2, 40
->
add $4, $5, $6
• <branch result>
<branch result>

Cycle Time Matters
• Delayed branches work when branches are short
• no processor uses a delayed branch of more than 1 cycle
• Longer branch delays rely on hardware based branch 
prediction

Reducing Branch Delay
• Insight:
• many branches rely only on simple tests 
• can be implemented with a few gates over an ALU
• more complex branches take two instructions
• slt followed by beq/bne
• Move hardware to determine outcome to ID stage
• Target address adder
• Register comparator
• Reduces the penalty of a branch to only one instruction if 
the branch is taken

Summary
• Pipelining improves performance by increasing instruction 
throughput
• Executes multiple instructions in parallel
• Each instruction has the same latency
• Subject to hazards
• Structure, data, control
• Instruction set design affects complexity of pipeline 
implementation

Exceptions and Interrupts
• “Unexpected” events requiring change in flow of 
control
• Exception
• Arises within the CPU
• Examples: undefined opcode, overflow, syscall
• Interrupt
• From an external I/O controller

Handling Exceptions
• In MIPS, exceptions managed by a System 
Control Coprocessor (CP0)
• Save PC of offending (or interrupted) instruction
• In MIPS: Exception Program Counter (EPC)
• Save indication of the problem
• In MIPS: Status register
• 0000 – undefined instruction
• 0180 – arithmetic overflow
• Jump to handler

Handler Actions
• Read cause, and transfer to relevant handler
• Determine action required
• If restartable
• Take corrective action
• use EPC to return to program
• Otherwise
• Terminate program
• Report error using EPC

Exceptions in a Pipeline
• Another form of control hazard
• Consider overflow on add in EX stage
add $1, $2, $1
• Prevent $1 from being overwritten
• Complete previous instructions
• Flush add and subsequent instructions
• Set Cause and EPC register values
• Transfer control to handler
• Similar to mispredicted branch
• Use much of the same hardware

Multiple Exceptions
• Pipelining overlaps multiple instructions
• Could have multiple exceptions at once
• Simple approach: deal with exception from 
earliest instruction
• Flush subsequent instructions

Fallacies
• Pipelining is easy
• The basic idea is easy
• The devil is in the details
• e.g., detecting data hazards
• Pipelining is independent of technology
• So why haven’t we always done pipelining?
• More transistors make more advanced techniques 
feasible
• Pipeline-related ISA design needs to take account of 
technology trends

Pitfalls
• Poor ISA design can make pipelining harder
• e.g., complex instruction sets (VAX, IA-32)
• Significant overhead to make pipelining work
• e.g., complex addressing modes
• Register update side effects, memory indirection
• e.g., delayed branches
• Advanced pipelines have long delay slots

Concluding Remarks
• ISA influences design of datapath and control
• Datapath and control influence design of ISA
• Pipelining improves instruction throughput
• More instructions completed per second
• Latency for each instruction not reduced
• Hazards: structural, data, control
• Exceptions are handled by a outside handler and 
control hazard logic.

