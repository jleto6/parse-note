
<style>
    body {
      background-color: #1e1e1e; /* or use 'darkgray' or '#2c2c2c' for other shades */
    }
  </style>


<h2 style="color:whitesmoke;">Memory Hierarchy: Caches</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">In an <strong>ideal computing system</strong>, there would be unlimited fast memory access. However, such a system is not practically achievable. Instead, a <strong>memory hierarchy</strong> provides the <strong>illusion of large amounts of fast memory</strong> by organizing different types of storage to optimize speed and accessibility. <!-- END_SECTION --></p>

<h3 style="color:whitesmoke;">Principle of Locality</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">The <strong>principle of locality</strong> refers to the observation that programs tend to access only a small portion of their address space at any given time, which is crucial to optimizing the memory hierarchy. There are two main types of locality:</p>

<ol style="color:whitesmoke;">
    <li><strong>Temporal Locality</strong>: Refers to the tendency of programs to access the same memory locations repeatedly within a short period. Examples include <strong>instructions in a loop</strong> and <strong>induction variables</strong>. <!-- END_SECTION --></li>
    <li><strong>Spatial Locality</strong>: Denotes the likelihood of accessing memory locations close to those recently accessed. This occurs commonly in <strong>sequential instruction access</strong> and accessing <strong>array data</strong>. <!-- END_SECTION --></li>
</ol>

<p style="color:whitesmoke;">Understanding these locality principles helps in designing cache systems and hierarchies to enhance performance. <!-- END_SECTION --></p>

<h3 style="color:whitesmoke;">Memory Hierarchy Levels</h3><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li><strong>Cache Levels</strong>: Include smaller, faster memory that is closer to the processor. Common technologies used are <strong>SRAM</strong>, <strong>DRAM</strong>, and <strong>Magnetic Disk</strong>. <!-- END_SECTION --></li>
    <li><strong>Memory Blocks</strong>: The unit of copying, which may consist of multiple words. This is a key concept in managing memory transitions between hierarchy levels. <!-- END_SECTION --></li>
</ul>

<p style="color:whitesmoke;">Caches are utilized to <strong>store frequently accessed data</strong> closer to the processor, minimizing access times and improving speed. In particular, cache memory attached directly to the CPU can significantly benefit from <strong>both temporal and spatial locality</strong>. <!-- END_SECTION --></p>

<h3 style="color:whitesmoke;">Cache Memory</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">A <strong>cache memory</strong> is the closest level of memory hierarchy to the CPU. It serves to swiftly retrieve the most frequently or recently accessed data. <!-- END_SECTION --></p>

<ol style="color:whitesmoke;">
    <li><strong>Hit</strong>: Occurs when accessed data is present in the upper memory level, leading to a quick data fetch. The <strong>hit ratio</strong> is a metric that denotes the number of hits divided by total accesses. <!-- END_SECTION --></li>
    <li><strong>Miss</strong>: Happens when data is absent in the cache, requiring a fetch from a lower memory tier, which is called a <strong>miss penalty</strong>. The <strong>miss ratio</strong> is calculated as one minus the hit ratio. <!-- END_SECTION --></li>
</ol>

<h3 style="color:whitesmoke;">Direct Mapped Cache</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">A <strong>direct mapped cache</strong> assigns each memory word to a specific, fixed cache location based on the <strong>block address modulo the number of cache blocks</strong>. This requires calculating where each piece of data resides within the cache system. Using the low-order address bits is crucial when the number of blocks is a power of two. <!-- END_SECTION --></p>

<h3 style="color:whitesmoke;">Tags and Valid Bits</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">To identify which block is stored in a specific cache location, caches use <strong>tags</strong>, which consist of the high-order bits from the address of the stored data. The <strong>valid bit</strong> indicates whether the data in a cache block is current; it defaults to zero on initialization and is set when data is stored. <!-- END_SECTION --></p>

<p style="color:whitesmoke;">Cache reads are notably straightforward because they do not alter the cache contents. For instance, consider an <strong>8-block direct mapped cache</strong> where addresses are managed as follows:</p>

<ul style="color:whitesmoke;">
    <li>Convert addresses to binary and determine their corresponding cache blocks using <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">address mod 8</code>. <!-- END_SECTION --></li>
</ul>

<p style="color:whitesmoke;">This approach ensures an efficient and orderly system for handling memory requests and cache storage operations, maintaining an optimally functioning memory hierarchy. <!-- END_SECTION --></p><h2 style="color:whitesmoke;">Cache Example Analysis</h2>
<p style="color:whitesmoke;">In examining cache behavior, it is essential to understand the concept of mapping between memory and cache blocks. Consider the following scenario:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>
        <strong>Word Address:</strong> This specifies the memory address of the word.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Binary Address:</strong> The binary representation of the word address, often crucial for index calculation.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Cache Block:</strong> Determines the specific block in the cache where the data might reside.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Index and Tag:</strong> Index is derived from the address to find the cache block, while tags help identify the particular block stored.<!-- END_SECTION -->
    </li>
</ul>

<p style="color:whitesmoke;">Initially, the address bits are subdivided to determine the specific location:</p><!-- END_SECTION -->

<p style="color:whitesmoke;">The address consists of several fields:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>
        <strong>Byte Offset:</strong> Determines the specific byte within a block.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Index:</strong> Used to find the cache block, the crucial part for index calculation. It specifies which cache line to access.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Tag:</strong> High-order bits used to check against stored data to ensure the correct block is present.<!-- END_SECTION -->
    </li>
</ul>

<h2 style="color:whitesmoke;">Cache Hit and Miss Analysis</h2>

<p style="color:whitesmoke;">A <strong>cache hit</strong> occurs when the data sought is present in the cache, determined by these conditions:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>
        If the <strong>valid bit</strong> indicates presence, tags are checked.<!-- END_SECTION -->
    </li>
    <li>
        Matching <strong>tags</strong> confirm a hit, allowing data usage from the cache.<!-- END_SECTION -->
</ul>

<p style="color:whitesmoke;">A <strong>cache miss</strong> happens when:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>
        The <strong>valid bit</strong> is not set or tags do not match. This requires further action by the <strong>control unit</strong>, creating a <strong>pipeline stall</strong>.<!-- END_SECTION -->
    </li>
</ul>

<h2 style="color:whitesmoke;">Cache Miss Handling</h2>

<ol style="color:whitesmoke;">
    <li>
        Send the original <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">PC</code> value (current PC â€” 4) to memory, initiating a read from main memory.<!-- END_SECTION -->
    </li>
    <li>
        Wait for memory access completion.<!-- END_SECTION -->
    </li>
    <li>
        Write to the cache entry, updating the <strong>data portion</strong> and setting the <strong>tag field</strong> and <strong>valid bit</strong>.<!-- END_SECTION -->
    </li>
    <li>
        Restart instruction execution, allowing the instruction to fetch from cache.<!-- END_SECTION -->
</ol>

<h2 style="color:whitesmoke;">Cache Miss Types and Mitigation</h2>

<ul style="color:whitesmoke;">
    <li>
        <strong>Compulsory Misses:</strong> First-time access data; unavoidable due to cold cache start.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Capacity Misses:</strong> Arise when the <strong>program's working set</strong> exceeds cache size, challenging cache locality management.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Conflict Misses:</strong> Caused by multiple blocks competing for the same cache space, reducible by altering cache associativity.<!-- END_SECTION -->
    </li>
</ul>

<p style="color:whitesmoke;">Different cache organizations play a vital role in how data is mapped and accessed, each providing unique trade-offs:</p><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Associativity and Cache Organizations</h2>

<ul style="color:whitesmoke;">
    <li>
        <strong>Direct Mapped:</strong> Simplest form of mapping, each block maps to a single cache location.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Set Associative:</strong> Allows each block to map to <strong>n</strong> locations. A flexible scheme mitigating collisions, determined by the block number modulo number of sets.<!-- END_SECTION -->
    </li>
    <li>
        <strong>Fully Associative:</strong> Blocks can map to any cache location. Though flexible, it requires parallel checking of all entries, increasing complexity.<!-- END_SECTION -->
    </li>
</ul>

<p style="color:whitesmoke;">These organizations are part of a spectrum. For instance,:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>
        <strong>For 8 entries:</strong> Varies from one-way set associative to eight-way (fully associative). The choice impacts <strong>hit rate</strong> and <strong>access time</strong>.<!-- END_SECTION -->
    </li>
</ul><h2 style="color:whitesmoke;">Set Associative Cache Organization</h2>
<p style="color:whitesmoke;">In a <strong>set associative cache</strong>, each address is subdivided into parts, such as <strong>index</strong> and <strong>tags</strong>. The index determines which set in the cache might contain the corresponding block. Each set can hold multiple data entries, offering more flexibility than a direct mapped cache and reduced complexity compared to a fully associative cache.<!-- END_SECTION --></p>

<h3 style="color:whitesmoke;">Replacement Policies</h3>
<ul style="color:whitesmoke;">
    <li>
        <strong>Direct mapped:</strong> There is no choice for replacement since each block has one specific location.
    </li><!-- END_SECTION -->
    <li>
        <strong>Set associative:</strong> Provides a few different replacement options:
        <ol style="color:whitesmoke;">
            <li>
                <strong>Non-valid entry:</strong> If one is available, it is preferred for new data.
            </li><!-- END_SECTION -->
            <li>
                <strong>Least-recently used (LRU):</strong> Replaces the entry that has not been used for the longest time. While simple for 2-way set associative caches, it becomes complex for associativity levels higher than 4-way.
            </li><!-- END_SECTION -->
            <li>
                <strong>Random:</strong> Offers similar performance to LRU for caches with high associativity, without the need to track access history.
            </li><!-- END_SECTION -->
        </ol>
    </li><!-- END_SECTION -->
</ul><!-- END_SECTION -->

<h3 style="color:whitesmoke;">Associativity Examples</h3>
<p style="color:whitesmoke;">Comparing cache types helps illustrate differences in behavior and efficiency. The examples below demonstrate how caches with varying associativity levels manage a sequence of block accesses.<!-- END_SECTION --></p>

<h4 style="color:whitesmoke;">Direct Mapped Cache Example</h4>
<ol style="color:whitesmoke;">
    <li>
        <strong>Block access sequence:</strong> 0, 8, 0, 6, 8
        <ul style="color:whitesmoke;">
            <li>Address 0: <strong>Miss</strong> (Mem[0] loaded)</li><!-- END_SECTION -->
            <li>Address 8: <strong>Miss</strong> (Mem[8] loaded, replacing Mem[0])</li><!-- END_SECTION -->
            <li>Address 0: <strong>Miss</strong> (Mem[0] reloaded, replacing Mem[8])</li><!-- END_SECTION -->
            <li>Address 6: <strong>Miss</strong> (Mem[6] loaded, replacing Mem[0])</li><!-- END_SECTION -->
            <li>Address 8: <strong>Miss</strong> (Mem[8] reloaded, replacing Mem[6])</li><!-- END_SECTION -->
        </ul><!-- END_SECTION -->
    </li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<h4 style="color:whitesmoke;">2-Way Set Associative Cache Example</h4>
<ol style="color:whitesmoke;">
    <li>
        <strong>Block access sequence:</strong> 0, 8, 0, 6, 8
        <ul style="color:whitesmoke;">
            <li>Address 0: <strong>Miss</strong> (Mem[0] loaded into first available spot in set)</li><!-- END_SECTION -->
            <li>Address 8: <strong>Miss</strong> (Mem[8] loaded into remaining spot in set)</li><!-- END_SECTION -->
            <li>Address 0: <strong>Hit</strong> (Mem[0] already in cache)</li><!-- END_SECTION -->
            <li>Address 6: <strong>Miss</strong> (Mem[6] replaces Mem[8] based on LRU policy)</li><!-- END_SECTION -->
            <li>Address 8: <strong>Miss</strong> (Mem[8] reloaded, replacing Mem[0])</li><!-- END_SECTION -->
        </ul><!-- END_SECTION -->
    </li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<p style="color:whitesmoke;">The variety of cache organizations, such as <strong>direct mapped</strong>, <strong>2-way set associative</strong>, and <strong>fully associative caches</strong>, each offer unique advantages and trade-offs, affecting how memory is accessed and efficiency in memory utilization. These configurations highlight contrasts in <strong>hit rates</strong> and require different strategies for <strong>block replacement</strong>.<!-- END_SECTION --></p><h2 style="color:whitesmoke;">Associativity Example</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">In a <strong>comparison</strong> of different cache organizations, we explore a <strong>4-block cache</strong> using <strong>direct mapped</strong>, <strong>2-way set associative</strong>, and <strong>fully associative</strong> configurations. This comparison examines the effects of block access sequence and miss rates.</p><!-- END_SECTION -->

<ol style="color:whitesmoke;">
    <li><strong>Direct Mapped Cache</strong>: In this configuration, each block can go in exactly one location in the cache. The access sequence is: 0, 8, 0, 6, 8.
        <ul style="color:whitesmoke;">
            <li>Accessing block 0: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Accessing block 8: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Accessing block 0 again results in a <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">hit</code>, as it is already in cache.</li><!-- END_SECTION -->
            <li>Accessing block 6: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Accessing block 8 again results in a <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">hit</code>.</li><!-- END_SECTION -->
        </ul><!-- END_SECTION -->
    </li><!-- END_SECTION -->
    <li><strong>2-Way Set Associative Cache</strong>: Each block can be placed in two different locations in the same set. The sequence remains: 0, 8, 0, 6, 8.
        <ul style="color:whitesmoke;">
            <li>Accessing block 0: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>, content is <strong>Mem[0]</strong>.</li><!-- END_SECTION -->
            <li>Accessing 8: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>, now <strong>Mem[0]</strong> and <strong>Mem[8]</strong> in cache.</li><!-- END_SECTION -->
            <li>Accessing 0 again is a <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">hit</code>.</li><!-- END_SECTION -->
            <li>Accessing 6: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>, evicting <strong>Mem[8]</strong>.</li><!-- END_SECTION -->
            <li>Accessing 8 is another <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
        </ul><!-- END_SECTION -->
    </li><!-- END_SECTION -->
    <li><strong>Fully Associative Cache</strong>: Any block can go into any location. The sequence continues as 0, 8, 0, 6, 8.
        <ul style="color:whitesmoke;">
            <li>Accessing block 0: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Accessing block 8: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Re-accessing 0 results in a <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">hit</code>.</li><!-- END_SECTION -->
            <li>Accessing 6: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">miss</code>.</li><!-- END_SECTION -->
            <li>Accessing 8 is a <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">hit</code>, as both <strong>Mem[8]</strong> and <strong>Mem[6]</strong> are retained.</li><!-- END_SECTION -->
        </ul><!-- END_SECTION -->
    </li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<h2 style="color:whitesmoke;">How Much Associativity Matters</h2><!-- END_SECTION -->

<p style="color:whitesmoke;"><strong>Increased associativity</strong> within a cache system reduces the <strong>miss rate</strong>, thus enhancing performance. However, the benefits show <strong>diminishing returns</strong> beyond certain thresholds. For instance, in a simulation of a 64KB cache with 16-word blocks, the miss rates were observed as follows:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>1-way set associative: 10.3%</li><!-- END_SECTION -->
    <li>2-way set associative: 8.6%</li><!-- END_SECTION -->
    <li>4-way set associative: 8.3%</li><!-- END_SECTION -->
    <li>8-way set associative: 8.1%</li><!-- END_SECTION -->
</ul><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Cache Write Mechanisms</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">When performing a <strong>cache write</strong>, consistency between the cache and main memory is crucial. A mismatch can lead to discrepancies. Thus, techniques like <strong>write-through</strong> and <strong>write-back</strong> are employed to ensure synchronization between these memory levels.</p><!-- END_SECTION -->

<h3 style="color:whitesmoke;">Write-Through</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">This technique involves updating both <strong>the cache and main memory</strong> concurrently. However, writes typically take longer under this scheme. With 10% of instructions being stores and writes taking 100 cycles, the effective <strong>Cycles per Instruction (CPI)</strong> increases. A solution is to use a <strong>write buffer</strong>, which holds data waiting to be written to memory.</p><!-- END_SECTION -->

<h3 style="color:whitesmoke;">Write-Back</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">In this method, updates occur in the cache first, and only when a block is replaced does it get written back to memory. Blocks are marked as either <strong>"clean"</strong> or <strong>"dirty"</strong> to track changes. This allows reduced write traffic but requires careful management of dirty blocks.</p><!-- END_SECTION -->

<p style="color:whitesmoke;"><strong>Write allocation</strong> decisions differ between policies: write-through may fetch the block on a miss (allocate on miss) or choose not to fetch it (write around), while write-back often involves fetching the block.</p><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Performance Considerations</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">Performance heavily depends on <strong>hit and miss penalties</strong>. The time to service these events impacts <strong>CPU time</strong>. Specifically, cache hit time is usually less than the miss penalty, thus optimizing cache structures to reduce miss penalties is vital.</p><!-- END_SECTION -->

<h3 style="color:whitesmoke;">Measuring Cache Performance</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">Cache performance can be measured by analyzing <strong>CPU time</strong>. It consists of:</p><!-- END_SECTION -->

<ol style="color:whitesmoke;">
    <li>Program execution cycles: Includes <strong>cache hit time</strong>.</li><!-- END_SECTION -->
    <li>Memory stall cycles: Arise mainly from cache misses.</li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<p style="color:whitesmoke;">The formula for calculating memory stall cycles is: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">Memory-stall cycles = Read-stall cycles + Write-stall cycles</code>. It further breaks down as:</p><!-- END_SECTION -->

<ul style="color:whitesmoke;">
    <li>Read-stall cycles: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">Reads/Program * Read miss rate * Read miss penalty</code></li><!-- END_SECTION -->
    <li>Write-stall cycles: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">(Writes/Program * write miss rate * write miss penalty) + Write buffer stalls</code></li><!-- END_SECTION -->
</ul><!-- END_SECTION -->

<p style="color:whitesmoke;">Consider an example with 2% instruction cache miss rate and 4% data cache miss rate. Assuming a Base CPI of 2 and a miss penalty of 100 cycles, where loads and stores make up 36%, the resulting memory miss cycles can be calculated as: 2 (I-cache) + 1.44 (D-cache) = 3.44.</p><!-- END_SECTION --><h2 style="color:whitesmoke;">Cache Performance Analysis</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">In evaluating <strong>cache performance</strong>, it is essential to consider both the <strong>instruction cache (I-cache) miss rate</strong> and the <strong>data cache (D-cache) miss rate</strong>. In a given example, the I-cache miss rate is 2% while the D-cache miss rate is 4%. The <strong>base CPI (Cycles Per Instruction)</strong> is 2, assuming an ideal cache with zero misses. The <strong>miss penalty</strong> for all misses in this scenario is 100 cycles. Finally, 36% of instructions involve loads and stores.</p><!-- END_SECTION -->

<ol style="color:whitesmoke;">
    <li>Calculate the <strong>miss cycles per instruction</strong> for the I-cache. Multiply the miss rate by the miss penalty: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">0.02 x 100 = 2</code>.</li><!-- END_SECTION -->
    <li>Compute the miss cycles for the D-cache given the percentage of load and store instructions: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">0.36 x 0.04 x 100 = 1.44</code>.</li><!-- END_SECTION -->
    <li>Add these results to find the total <strong>memory miss cycles</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">2 + 1.44 = 3.44</code>.</li><!-- END_SECTION -->
    <li>Determine the <strong>actual CPI</strong> by adding the base CPI to the total memory miss cycles: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">2 + 3.44 = 5.44</code>.</li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<p style="color:whitesmoke;">The <strong>ideal CPU</strong> in this example (assuming a theoretical perfect cache) would be <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5.44 / 2 = 2.72</code> times faster compared to the non-ideal situation.</p><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Performance Considerations</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">If the <strong>processor speed</strong> is increased without a corresponding improvement in the <strong>memory system</strong>, the <strong>time spent on memory stalls</strong> will make up a larger fraction of the execution time. This effect is explained by <strong>Amdahl's Law</strong>, which illustrates the diminishing returns of improvements that do not affect the entire process.</p><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Revisiting Cache Performance</h2><!-- END_SECTION -->

<ol style="color:whitesmoke;">
    <li>Suppose the system's CPI is halved from 2 to 1 without altering the clock rate.</li><!-- END_SECTION -->
    <li>Calculate the new <strong>actual CPI</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1 + 3.44 = 4.44</code>.</li><!-- END_SECTION -->
    <li>The <strong>ideal performance increase</strong> is now: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">4.44 / 1 = 4.44</code> times faster.</li><!-- END_SECTION -->
    <li>The proportion of time spent on memory stalls before and after the CPI change are <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">63%</code> and <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">77%</code>, respectively.</li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<h2 style="color:whitesmoke;">Multilevel Caches</h2><!-- END_SECTION -->

<p style="color:whitesmoke;">A <strong>multilevel cache</strong> system introduces a <strong>Level-2 (L-2) cache</strong> to alleviate primary cache misses. The <strong>primary cache</strong> is attached directly to the CPU and is designed for speed. The L-2 cache handles misses from the primary cache and is larger and slower, yet provides faster access than main memory. Furthermore, some systems incorporate an additional <strong>Level-3 (L-3) cache</strong>.</p><!-- END_SECTION -->

<h3 style="color:whitesmoke;">Multilevel Cache Example</h3><!-- END_SECTION -->

<p style="color:whitesmoke;">Consider a processor with a base CPI of 1.0 and a clock rate of 5 GHz, with a miss rate of 2% at the primary cache. The main memory access time is 100 ns. Introducing an L-2 cache reduces the main memory miss rate to 0.5% and has an access time of 5 ns.</p><!-- END_SECTION -->

<ol style="color:whitesmoke;">
    <li>Calculate the <strong>miss penalty</strong> with only a primary cache: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">100 ns / 0.2 ns = 500 cycles</code>.</li><!-- END_SECTION -->
    <li>Establish the <strong>effective CPI</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">1 + 0.02 x 500 = 11</code>.</li><!-- END_SECTION -->
    <li>Evaluate the <strong>performance gain</strong> with the L-2 cache. Calculate the primary miss with an L-2 hit penalty: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">5 ns / 0.2 ns = 25 cycles</code>.</li><!-- END_SECTION -->
    <li>Compute the <strong>Overall Performance Ratio</strong>: <code style="color:#00aaff; font-family: Menlo, Monaco, 'Courier New', monospace;">11 / 4 = 2.8</code>.</li><!-- END_SECTION -->
</ol><!-- END_SECTION -->

<p style="color:whitesmoke;">In a multilevel cache setup, the primary cache minimizes hit time, while the L-2 cache reduces the miss rate, impacting the overall access time less critically.</p><!-- END_SECTION -->